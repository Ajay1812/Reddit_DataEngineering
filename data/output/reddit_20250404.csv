id,title,selftext,score,upvote_ratio,num_comments,author,created_utc,url,over_18,edited,spoiler,stickied
1jr15ej,What's the non-technical biggest barrier you face at work?,"What’s currently challenging for me is getting access to things.

I design a data pipeline, present it to the team that will benefit from it, and everyone gets super excited.

Then I reach out to the internal department or an external party to either grant me admin access to the platform I need, or to help me obtain an API.

A week goes by—nothing. I follow up via email. Eventually, someone replies and says it's not possible to give me admin credentials. Fine. So I ask, “Can you help me get the API instead? It’s very straightforward.”

Another week goes by—still nothing. I send another follow-up…

Now the other person is kind of frustrated (because I’m asking them to do something slightly different, even though I’m offering guidance).

What follows is just a back-and-forth with long, frustrating waiting periods in between. Meanwhile, the team I presented the pipeline or project to starts getting frustrated with me and probably thinks I’m full of crap.

Once I finally get the damn API or whatever access I needed, I complete the project in 1–2 days but delayed by weeks or even months.

Aaaaaaah!",37,0,14,sirtuinsenolytic,2025-04-04 02:23:50,https://www.reddit.com/r/dataengineering/comments/1jr15ej/whats_the_nontechnical_biggest_barrier_you_face/,False,False,False,False
1jqtpfq,When do you expect a mid level to be productive?,"I recently started a new position as a mid-level Data Engineer, and I feel like I’m spending a lot of time learning the business side and getting familiar with the platforms we use.

At the same time, the work I’m supposed to be doing is still being organized.

In the meantime, I’ve been given some simple tasks, like writing queries, to work on—but I can’t finish them because I don’t have enough context.

I feel stressed because I’m not solving fundamental problems yet, and I’m not sure if I should just give it more time or take a different approach.",24,0,15,idleAndalusian,2025-04-03 20:44:36,https://www.reddit.com/r/dataengineering/comments/1jqtpfq/when_do_you_expect_a_mid_level_to_be_productive/,False,False,False,False
1jr68kn,Are Hyperscalers becoming more expensive in Europe due to the tariffs?,"Hi,

With the recent tariffs in mind, are cloud providers like AWS, Azure, and Google Cloud becoming more expensive for European companies? And what about other techs like Snowflake or Databricks – are they affected too?

Would it be wise for European businesses to consider open-source alternatives, both for cost and strategic independence?

And from a personal perspective: should we, as employees, expand our skill sets toward open-source tech stacks to stay future-proof?",21,0,21,Ok-Inspection3886,2025-04-04 07:31:32,https://www.reddit.com/r/dataengineering/comments/1jr68kn/are_hyperscalers_becoming_more_expensive_in/,False,False,False,False
1jqrxyq,Open source alternatives to Fabric Data Factory,"Hello Guys,

We are trying to explore open-source alternatives to Fabric Data Factory. Our sources main include oracle/MSSQL/Flat files/Json/XML/APIs..Destinations should be Onelake/lakehouse delta tables?

I would really appreciate if you have any thoughts on this?

Best regards :) 

",14,0,10,Harshadeep21,2025-04-03 19:36:58,https://www.reddit.com/r/dataengineering/comments/1jqrxyq/open_source_alternatives_to_fabric_data_factory/,False,False,False,False
1jqnphu,Code Exams - Tips from a hiring manager,"I previously founded and ran a team of 8 as Director of Data Engineering & BI at a small consulting company, and I currently consult freelance through my own LLC (where I occasionally hire subcontractors).

I wanted to share feedback to hopefully help some folks be successful with their Data Engineering code exams, especially in this economy.

Below are my tips and tricks that would make *any* candidate stand out from the pack, even if they don't get the technical answer right, and even if they are very junior in their experience.

I obviously can't claim to know what every other hiring manager might prioritize, but I would propose that any *good* hiring manager worth their salt is going to feel fairly similar to what I'm sharing below. 

## What I'm Looking For 

I don't care all that much about whether a candidate gets the technical answers right. They need to demonstrate a base-level of technical skills, to be sure, but that's it.

What I'm prioritizing is ""How do they solve problems?"" and what I'm looking for is the following: 

**1) Are They Defining & Solving the Right Problem**

Most of us are technical nerds that enjoy writing elegant/efficient code, but the best Data Engineers know how to evaluate whether the problem they're solving is actually the right problem to solve, and if not - how to dig deeper, identify root cause issues, escalate any underlying problems they see, and align with the priorities of leadership. 

**2) Can They Think Creatively?** 

When setting out to solve a problem, unless it's a well-defined problem with a well-understood solution (i.e. based on industry best practices), I expect good Data Engineers to come up with at least 2 to 3 different ways to solve the problem. Could be different tech stacks, diff programming languages, different algorithms... but I want to see creative, out-of-the-box thinking across multiple potential solution approaches.

**3) Can They Choose the Right Approach?** 

After sketching a few approaches to the problem, can the candidate identify the constraints and tradeoffs between each approach? Which is easiest to implement? Which is cheapest? Which is most maintainable in the long run? Which is the best performing? And what might limit/constrain each approach (time, cost, complexity, etc.)? A good Data Engineer will evaluate multiple solutions approaches across tradeoffs to decide on an ""optimal"" solution. A great Data Engineer will ensure that the tradeoffs they're considering are aligned with the priorities of their leadership & organization. 

So, in each problem in a code exam, if they can ""show their work"" across the points above, they will be way more competitive *even* if they get the technical answer wrong. 

##Other Considerations

**Attention to Detail**

I won't ask candidates if they have good ""attention to detail"" because everyone will claim they do. Instead, I'll structure my exam in such a way that they won't be successful unless they pick up on the details.

**Resourcefulness**

I will give candidates a lot of leeway if they come up with the wrong answers, if they can demonstrate resourcefulness. If I know I can give them a problem, and know that they'll figure it out ""one way or the other"" - I'll hire them over a technical expert who isn't otherwise resourceful.

**Ask Questions**

I will also prioritize candidates who ask (good) questions. I often mention in the code exams to ask questions if they're confused about anything, and I'll ensure the code exam has some ambiguity in it. Candidates who ask for clarification demonstrate some implicit humility, a capacity for critical thinking, a deliberate approach to solving the right problem, and much better reflect real-world projects that require navigating ambiguity.

Hope this is all somewhat helpful to candidates currently working through code exams!

Edit: Formatting, grammar, spelling",11,0,3,jodyhesch,2025-04-03 16:56:26,https://www.reddit.com/r/dataengineering/comments/1jqnphu/code_exams_tips_from_a_hiring_manager/,False,False,False,False
1jqwdid,What other jobs do you to liken DE to?,"What job / profession do you use to compare to DE, joking or not? 

A few favorites around my workplace: butcher, designer, baker, cook, alchemist, surgeon, magician, wizard, wrangler, gymnast, shepherd, unfucker, plumber

  
What are yours?",6,0,31,AgileGentile,2025-04-03 22:33:53,https://www.reddit.com/r/dataengineering/comments/1jqwdid/what_other_jobs_do_you_to_liken_de_to/,False,False,False,False
1jqscpa,Is the entry level barrier high for DE than SWE?,"Hello, 
I am interested in your opinions on the entry level of DE vs entry level of SWE interms of skillset width and depth. Do you consider breaking into DE is easier or tougher than SWE? Pros and Cons of entry level as well.

Solely interested in understanding what the community thinks as I have a couple of friends who want to move to DE and vice versa, ""because that's a great career"".",6,0,14,Dark_Man2023,2025-04-03 19:52:56,https://www.reddit.com/r/dataengineering/comments/1jqscpa/is_the_entry_level_barrier_high_for_de_than_swe/,False,False,False,False
1jqmfjq,Beyond Batch: Architecting Fast Ingestion for Near Real-Time Iceberg Queries,,8,0,0,Ill_Force756,2025-04-03 16:06:55,https://www.e6data.com/blog/architecting-fast-ingestion-real-time-iceberg-queries,False,False,False,False
1jr70yg,Which tool do you use to move data from the cloud to Snowflake?,"Hey, r/dataengineering 

I’m working on a project where I need to move data from our cloud-hosted databases into Snowflake, and I’m trying to figure out the best tool for the job. Ideally, I’d like something that’s cost-effective and scales well. 

If you’ve done this before, what did you use?
Would love to hear about your experience—how reliable it is, how much it roughly costs, and any pros/cons you’ve noticed. Appreciate any insights!

[View Poll](https://www.reddit.com/poll/1jr70yg)",5,0,8,Many-Tart-7661,2025-04-04 08:30:40,https://www.reddit.com/r/dataengineering/comments/1jr70yg/which_tool_do_you_use_to_move_data_from_the_cloud/,False,False,False,False
1jr3z1u,Faster way to view + debug data,"Hi r/dataengineering!

  
I wanted to share a project that I have been working on. It's an intuitive data editor where you can interact with local and remote data (e.g. Athena & BigQuery). For several important tasks, it can speed you up by 10x or more. (see website for more)

  
For data engineering specifically, this would be really useful in debugging pipelines, cleaning local or remote data, and being able to easy create new tables within data warehouses etc.

I know this could be a lot faster than having to type everything out, especially if you're just poking around. I personally find myself using this before trying any manual work.

Also, for those doing complex queries, you can split them up and work with the frame visually and add queries when needed. Super useful for when you want to iteratively build an analysis or new frame ***without writing a super long query.***

  
As for data size, it can handle local data up to around 1B rows, and remote data is only limited by your data warehouse.

  
You don't have to migrate *anything* either.

  
If you're interested, you can check it out here: [https://www.cocoalemana.com](https://www.cocoalemana.com)

  
I'd love to hear about your workflow, and see what we can change to make it cover more data engineering use cases.

  
Cheers!

[Coco Alemana](https://preview.redd.it/02wogjj72rse1.jpg?width=3820&format=pjpg&auto=webp&s=0905bd40927b4dd7e80521568982ebe82994a5fe)

",5,0,2,Impressive_Run8512,2025-04-04 05:00:10,https://www.reddit.com/r/dataengineering/comments/1jr3z1u/faster_way_to_view_debug_data/,False,False,False,False
1jr6a6h,How to stream results of a complex SQL query,"Hello,

I'm writing you because I have a problem with a side project and maybe here somebody can help me. I have to run a complex query with a potentially high number of results and it takes a lot of time. However, for my project I don't need all the results to be showed together, perhaps after some hours/days. It would be much more useful to get a stream of the partial results in real time. How can I achieve this? I would prefer to use free software, however please suggest me any solution you have in mind.

Thank you in advance!",4,1,10,forevernevermore_,2025-04-04 07:35:07,https://www.reddit.com/r/dataengineering/comments/1jr6a6h/how_to_stream_results_of_a_complex_sql_query/,False,False,False,False
1jr23mk,How do I get out of this rut,"I’m currently about the finish an early career rotational program with a top 10 bank. The rotation I am currently on and where the company is placing me post program (I tried to get placed somewhere else) is as a data engineer on a data delivery team. When I was advertised this rotation and the team I was told pretty specifically we would be using all the relevant technologies and I would be very hands on keyboard building pipelines with python , configuring cloud services and snowflake, being a part of data modeling. Mind you I’m not completely new I have experience with all this in personal projects and previous work experience as a SWE and researcher in college. 

Turns out all of that was a lie. I later learned there is an army of contractors that do the actual work. I was stuck with analyzing .egp files and other SAS files documenting it and handing off to consultants to rebuild in Talend to ingest into snowflake. The only tech that I use is Visio and Word.

I coped with that by saying after I’m out of the program I’ll get to do the actual work. But I had a conversation with my manager today about what my role will be post program. He basically said there are a lot more of these SAS procedures they are porting over to talend and snowflake and I’ll be documenting them and handing over to contractors so they can implement the new process. Honestly that is all really quick and easy to do because there isn’t that much complicated business logic for the LOBs we support just joins and the occasional aggregation so most days I’m not doing anything.

When I told him I would really like to be involved in the technical work or the data modeling , he said that is not my job anymore and that is what we pay the contractors to do so I can’t do it. Almost made it seem like I should be grateful and he is doing me a favor somehow.

It just feels like I was misled or even outright lied to about the position. We don’t use any of the technologies that were advertised (Drag and drop/low code tools seem like fake engineering), I don’t get to be hands on keyboard at all. Just seems like there really I no growth or opportunity in this role. I would leave but I took relocation and a signing bonus for this and if I leave too early I owe it back. I also can’t internally transfer anywhere for a year after starting my new role.

I guess my rant is just to ask what should I be doing in this situation? I work on personal projects and open source and I have gotten a few certs in the downtime at work but I don’t know if it’s enough to make sure my skills don’t atrophy while I wait out my repayment period. I consider myself a somewhat technical guy but I have been boxed into a non technical role.

",3,0,6,anonymous_0618615740,2025-04-04 03:13:53,https://www.reddit.com/r/dataengineering/comments/1jr23mk/how_do_i_get_out_of_this_rut/,False,False,False,False
1jrd286,Just wanted to share a recent win that made our whole team feel pretty good.,"We worked with this e-commerce client last month (kitchen products company, can't name names) who was dealing with data chaos.

When they came to us, their situation was rough. Dashboards taking forever to load, some poor analyst manually combining data from 5 different sources, and their CEO breathing down everyone's neck for daily conversion reports. Classic spreadsheet hell that we've all seen before.

We spent about two weeks redesigning their entire data architecture. Built them a proper [**data warehouse solution** ](https://datafortune.com/services/enterprise-data-management/data-warehouse/)with automated ETL pipelines that consolidated everything into one central location. Created some logical data models and connected it all to their existing BI tools.

The transformation was honestly pretty incredible to watch. Reports that used to take hours now run in seconds. Their analyst actually took a vacation for the first time in a year. And we got this really nice email from their CTO saying we'd ""changed how they make decisions"" which gave us all the warm fuzzies.

It's projects like these that remind us why we got into this field in the first place. There's something so satisfying about taking a messy data situation and turning it into something clean and efficient that actually helps people do their jobs better.",0,0,9,DataMaster2025,2025-04-04 14:20:44,https://www.reddit.com/r/dataengineering/comments/1jrd286/just_wanted_to_share_a_recent_win_that_made_our/,False,False,False,False
1jramqt,"Airbyte Connector Builder now supports GraphQL, Async Requests and Custom Components","Hello, Marcos from the Airbyte Team.

For those who may not be familiar, Airbyte is an open-source data integration (EL) platform with over 500 connectors for APIs, databases, and file storage.

In our last release we added several new features to our no-code Connector Builder:

* [GraphQL Support](https://docs.airbyte.com/connector-development/config-based/understanding-the-yaml-file/request-options#graphql-request-injection): In addition to REST, you can now make requests to GraphQL APIs (and properly handle pagination!)
* [Async Data Requests](https://docs.airbyte.com/connector-development/connector-builder-ui/async-streams): There are some reporting APIs that do not return responses immediately. For instance, with Google Ads.  You can now request a custom report from these sources and wait for the report to be processed and downloaded.
* [Custom Python Code Components](https://docs.airbyte.com/connector-development/connector-builder-ui/custom-components): We recognize that some APIs behave uniquely—for example, by returning records as key-value pairs instead of arrays or by not ordering data correctly. To address these cases, our open-source platform now supports custom Python components that extend the capabilities of the no-code framework without blocking you from building your connector.

We believe these updates will make connector development faster and more accessible, helping you get the most out of your data integration projects.

We understand there are discussions about the trade-offs between no-code and low-code solutions. At Airbyte, transitioning from fully coded connectors to a low-code approach allowed us to maintain a large connector catalog using standard components.  We were also able to create a better build and test process directly in the UI. Users frequently give us the feedback that the no-code connector Builder enables less technical users to create and ship connectors. This reduces the workload on senior data engineers allowing them to focus on critical data pipelines.

Something else that has been top of mind is speed and performance. With a robust and stable connector framework, the engineering team has been dedicating significant resources to introduce concurrency to enhance sync speed. You can read this[ blog post](https://airbyte.com/blog/improving-connector-sync-speed-up-to-10x-faster) about how the team implemented concurrency in the Klaviyo connector, resulting in a speed increase of about 10x for syncs.

I hope you like the news! Let me know if you want to discuss any missing features or provide feedback about Airbyte.",4,0,2,marcos_airbyte,2025-04-04 12:26:58,https://www.reddit.com/r/dataengineering/comments/1jramqt/airbyte_connector_builder_now_supports_graphql/,False,False,False,False
1jr1r2t,"Built a real-time e-commerce data pipeline with Kinesis, Spark, Redshift & QuickSight — looking for feedback","I recently completed a real-time ETL pipeline project as part of my data engineering portfolio, and I’d love to share it here and get some feedback from the community.

# What it does:

* Streams transactional data using **Amazon Kinesis**
* Backs up raw data in **S3** (Parquet format)
* Processes and transforms data with **Apache Spark**
* Loads the transformed data into **Redshift Serverless**
* Orchestrates the pipeline with **Apache Airflow (Docker)**
* Visualizes insights through a **QuickSight dashboard**

# Key Metrics Visualized:

* Total Revenue
* Orders Over Time
* Average Order Value
* Top Products
* Revenue by Category (donut chart)

I built this to practice real-time ingestion, transformation, and visualization in a scalable, production-like setup using AWS-native services.

# GitHub Repo:

[https://github.com/amanuel496/real-time-ecommerce-etl-pipeline](https://github.com/amanuel496/real-time-ecommerce-etl-pipeline)

If you have any thoughts on how to improve the architecture, scale it better, or handle ops/monitoring more effectively, I’d love to hear your input.

Thanks!",6,0,5,MysteriousRide5284,2025-04-04 02:55:36,https://www.reddit.com/r/dataengineering/comments/1jr1r2t/built_a_realtime_ecommerce_data_pipeline_with/,False,False,False,False
1jqqi0o,Data synergy across product portfolio,"Has anyone worked on a shippable data-powered product where ""1 + 1 = 3""?

Context: I'm an SE selling cloud data lake / data warehouse tools. The vertical I sell to (cybersecurity) is currently experiencing a wave of M&A and roll-ups. Customer product portfolios are integrated from a commercials perspective (get your network protection, endpoint protection, and cloud protection from one vendor). Even if the products are integrated from a UI perspective, they are still siloed from a data perspective. 

My intuition tells me that if our customers combined data across domains (say network, cloud, end point), they could create a smarter product / platform. 

Does this pass the sniff test with the data product builders on this sub? As a vendor, bigger better data warehouses are better (especially if they get built on my company's products). And more data is better for CRMs, LLMs, etc. where users have more data at their fingertips?

Where have bigger better data warehouses enabled the building and shipping smarter products?",3,0,0,zachattach32,2025-04-03 18:41:58,https://www.reddit.com/r/dataengineering/comments/1jqqi0o/data_synergy_across_product_portfolio/,False,False,False,False
1jqoog3,How to build UV-project into a Dockerimage with an external (local) package?,"https://preview.redd.it/sbmotdtzmnse1.png?width=434&format=png&auto=webp&s=744e3d5c3dd0ff6273b8f109df450276c276bcab

Hi all. I'm turning to you as I cant figure this out.

https://preview.redd.it/sv512lr6onse1.png?width=483&format=png&auto=webp&s=4394911bc1e758446be5b5fb757a3d80ec718e48

My flow1 pyproject.toml file is defined as such:

`name = ""flow1""`

`version = ""0.1.0""`

`description = ""Add your description here""`

`readme = ""README.md""`

`requires-python = "">=3.13""`

`dependencies = [`

`""dadjokes>=1.3.2"",`

`""prefect[docker]>=3.3.1"",`

`""utilities"",`

`]`

`[tool.uv.sources]`

`utilities = { path = ""../utilities"" }`

`[build-system]`

`requires = [""hatchling""]`

`build-backend = ""hatchling.build""`

`[tool.hatch.build.targets.wheel]`

`packages = ["".""]`

When I develop, utilities are available, but I cannot seem to build it into the Dockerimage in flow1. I followed the guides at [https://docs.astral.sh/uv/guides/integration/docker/#intermediate-layers](https://docs.astral.sh/uv/guides/integration/docker/#intermediate-layers), but it can never ""find"" utilities. I assume its because its not available inside the Dockerimage, so how can I solve that?

Can I add a build step separately? Usually it compiles when using uv sync.",1,0,6,DeepFryEverything,2025-04-03 17:33:09,https://www.reddit.com/r/dataengineering/comments/1jqoog3/how_to_build_uvproject_into_a_dockerimage_with_an/,False,False,False,False
1jqlb48,Can you suggest a flexible ETL incremental replication tool that integrates with other systems?,"I am currently designing a DWH architecture.

For this project, I need to extract a large amount of data from various sources, including a Postgres db with multiple shards, Salesforce, and Jira. I intend to use Airflow for orchestration, but I am not particularly fond of using it as a worker, also  CDC for PostgreSQL and Salesforce can be quite challenging and difficult to implement.

Therefore, I am seeking a flexible, robust tool with CDC support and good performance, especially for PostgreSQL, where there is a significant amount of data. It would be ideal if the tool supported an infinite data stream. Although I found an interesting tool called [ETL Works](https://etlworks.com/), but it seems to be a noname, and its performance is questionable, as they do not offer pricing based on performance.

If you have any suggestions or solutions that you think may be relevant, please let me know.  
Any criticism, comments, or other feedback is welcome.

Note: DWH db would be GreenPlum",3,0,9,Extreme-Childhood330,2025-04-03 15:23:21,https://www.reddit.com/r/dataengineering/comments/1jqlb48/can_you_suggest_a_flexible_etl_incremental/,False,False,False,False
1jqzz3y,General question about data consulting,"Let's say there's a data consulting company working within a certain industry (e.g., utilities or energy). How do they gain access to their clients' databases if they want to perform ETL or other services? How about working with their data in a cloud setting (e.g., AWS)? What is the usual process for that? Is the consulting company responsible for setting and managing AWS costs, etc.?",3,0,7,No7-Francesco88,2025-04-04 01:23:44,https://www.reddit.com/r/dataengineering/comments/1jqzz3y/general_question_about_data_consulting/,False,False,False,False
1jqr20e,Installing spark from official website VS Installing pyspark library using pip,"Hi Folks,

Basically the title : What's the difference between installing spark from official website VS Installing pyspark library using pip. Are they one and the same thing or there is some difference ? 

Thanks in advance !! ",2,0,0,RC-05,2025-04-03 19:03:00,https://www.reddit.com/r/dataengineering/comments/1jqr20e/installing_spark_from_official_website_vs/,False,False,False,False
1jqp9jq,"Data model & tool stack for small, frequently changing dataset with many diverse & changing text attributes?","SQL / DW / BI dinosaur here tapped by a friend to help design a data model for a barebones bootstrapped MVP. 0 experience with NoSQL, or backend AI/ML other than being an end-user of it, but eager to ramp up quickly.

Friend has a small, frequently changing set of data with many diverse text attributes, a couple of them numerical for filtering based on simple math. The original formats of the data sources they want to pull in from is all over the place: tabular, written out in shortened sentences or paragraphs, etc. Friend took the time & effort to human-parse & codify the data into 2 formats: table & matrix. However, it took more time & effort than friend would prefer.

We would need to adapt to frequent schema and query changes. A couple of ways to design this relationally would be with wide tables, a lot of lookups (with perhaps lots of nested lookups), or something in between, which are constantly changing.

End-user usage patterns would involve very frequent querying of this data, either via an online form, or by scanning documents or screens provided by the end-user which may also have a variety of different formatting to them, or possibly via a chatbot. Querying and retrieval needs to be as contextually accurate as possible.

Considering recent ML/AI advancements, we're wondering of such an approach would be more efficient than a traditional MVC approach? My extremely limited understanding of ML/AI at this point is that larger datasets would help reinforce training a model. If we're constrained by a small dataset of no more than a few thousand records, then an ML backend wouldn't make sense. Let me know if I'm mistaken.

As a single developer bootstrapping this project, an ideal solution would minimize engineering overhead and allows for rapid iteration.

Any pointers would be helpful for me to get up to speed. Thanks in advance.

Update: gonna look take a look at pgvector ",2,0,0,issai,2025-04-03 17:55:26,https://www.reddit.com/r/dataengineering/comments/1jqp9jq/data_model_tool_stack_for_small_frequently/,False,False,False,False
1jqnwlc,"Climbed from Jr to Staff in 2 years, but still paid peanuts—should I quit? (Panic attacks, US job offers, and a proposal in Hawaii… Lost)","Hi everyone, I’m here to ask for advice, hear your opinions, and vent my frustrations.

I work for a large automotive group and have been with them for less than two years as an outsourced employee based in Mexico. I started in a change management role, where I reviewed design modifications during vehicle development. Four months in, three of my colleagues were laid off, and their workload was assigned to me. By then, I had already automated my entire workflow using Python, a process that was previously manual and took days, reducing my daily tasks to just 30 minutes.

The organization noticed my contributions and transferred me to a global solutions implementation team. In a short time, I rotated through three different groups: economic data analytics, IT, and data science. I became an expert in Palantir Foundry (pipelines, dashboards, etc.) and eventually led the team that was once above me (People with at least 10+ years in their current role). I went from Junior to Staff-level in under two years, yet my salary and conditions haven’t improved at all.

My outsourcing company promised to adjust my pay based on my responsibilities, and the automotive firm pledged to sponsor me for a role in Europe or the U.S. However, it’s been a year since those promises were made (They said this change takes no more than 2 months). I follow up every two weeks, but my outsourcing employer has even threatened to penalize me for ""unethical persistence."", also I know that the purchase order for my services has been paid several months ago and the outsourcing company have the money to pay my new salary.

My frustration stems from earning \~$24K USD/year in Mexico, while local market rates for my skills are up to 4x higher, and international roles pay 10x more. I’ve applied to numerous data engineer, analyst, and scientist roles domestically and abroad, but I keep hitting the same wall: ""Not enough years of experience"" (typically 8–12 required). Though I have 6 years of total experience (only 2 verifiable in IT/software engineering at 28 years old), my bachelor’s and master’s degrees are unrelated to programming—I’m entirely self-taught in data fields over the past 3 years.

Recently, I’ve received U.S. job offers for Palantir- and Databricks-related roles with strong salaries (130K–210K USD). Interviews go well until the final rounds, where I’m told:

* *""You lack seniority."" (why they call in the first place? lol)*
* *""You need X programming language.""*
* *""Your degree isn’t relevant.""*

Despite architecting the company’s economic tools and leading initiatives, I struggle with imposter syndrome. I learned everything independently—no paid courses—and often feel unprepared in interviews.

I need your advice: If my current employer won’t improve my conditions, what should I do? I’m lost, overwhelmed, and recently had panic attacks severe enough to require hospitalization. On top of this, I’m proposing to my girlfriend during a trip to Hawaii in May.

Thank you for reading—I’d truly appreciate your thoughts.",2,0,20,proximads,2025-04-03 17:03:47,https://www.reddit.com/r/dataengineering/comments/1jqnwlc/climbed_from_jr_to_staff_in_2_years_but_still/,False,False,False,False
1jre0v2,Logging in Spark applications.,"Hi guys, i am moving to on-prem managed Spark applications with Kuberenetes. I am wondering what do u use for logging? I am talking about Python and PySpark. Do u setup log4j? Or just use Python's logging library for application? What is the standard here? I have not seen much about log4j within PySpark.",3,1,0,Hot_While_6471,2025-04-04 15:01:43,https://www.reddit.com/r/dataengineering/comments/1jre0v2/logging_in_spark_applications/,False,False,False,False
1jrdue4,Anyone know of any vscode linter for sql that can accommodate pyspark sql?,"In pyspark 3.4 you can write sql as 

spark.sql(SELECT * FROM {df_input}, df_input = df_input) 

The popular sql linters I tried SQL Formatter and and Prettier SQL Vscode currently does not accommodate{}. Does anyone know of any linters that does? Thank you",2,1,0,AUGcodon,2025-04-04 14:54:19,https://www.reddit.com/r/dataengineering/comments/1jrdue4/anyone_know_of_any_vscode_linter_for_sql_that_can/,False,False,False,False
1jrd8po,PII Obfuscation in Databricks,"Hi Data Champs,

I have been recently given chance to explore PII obfuscation technique in databricks.

I proposed using sql aes_encryption or python fernet for PII column level encryption before landing to bronze.

And use column masking on delta tables which has built in logic for group membership check and decryption so to avoid the overhead of a new view per table.

My HDE was more interested in sql approach than the fernet but fernet offers built in key rotation out of the box.

Has anyone used aes_encryption 
Is it secure, easy to work with and relatively more robust.

From my experience for data type other than binary like long, int, double it needs to be first converted to binary (don’t like it)

Apart from that usual error here and there for padding and generic error when decrypting sometimes.

So given the choice what will be your architecture 

What you will prefer, what you don’t and why

I am open to DM if you wanna 💬 ",1,0,1,Intelligent-Mind8510,2025-04-04 14:28:31,https://www.reddit.com/r/dataengineering/comments/1jrd8po/pii_obfuscation_in_databricks/,False,False,False,False
1jrd0k2,Great Expectations Implementation,"Our company is implementing data quality testing and we are interested in borrowing from the Great Expectations suite of open source tests. I've read mostly negative reviews of the initial implementation of Great Expectations, but am curious if anyone else set up a much more lightweight configuration?

Ultimately, we plan to use the GX python code to run tests on data in Snowflake and then make the results available in Snowflake. Has anyone done something similar to this?",1,0,0,HAKOC534,2025-04-04 14:18:53,https://www.reddit.com/r/dataengineering/comments/1jrd0k2/great_expectations_implementation/,False,False,False,False
1jrc62b,Can you call an aimless star schema a data mart?,"So,

  
as always that's for the insight from other people, I find a lot of these discussions around points very entertaining and very helpful!

I'm having an argument with someone who is several levels above me. This might sound petty so I apologise in advance. It centres around the definition of a Mart. Our Mart is a single Fact with around 20 dimensions. The Fact is extremely wide and deep. Indeed we usually put it into a de normalised table for reporting. To me this isn't a MART as it isn't based on requirements but rather a star schema that supposedly servers multiple purposed or potential purposes. When engaged on requirements the person leans on there experience in the domain and says a user probable wants to do X, Y and Z. I've never seen anything written down. Constantly that report also defers to Kimball methodology and how this follows them closely. My take on the book is that these things need to be based of requirement, business requirements. 

My questions is, is it fair to say that a data mart needs to have requirements and ideally a business domain in mind or else its just a star schema?

Yes this  is  very theoretical... yes I probable need a hobby but look there hasn't been a decent RTS game in years and its friday!!!

Have a good weekend everyone",1,0,2,ObjectiveAssist7177,2025-04-04 13:41:31,https://www.reddit.com/r/dataengineering/comments/1jrc62b/can_you_call_an_aimless_star_schema_a_data_mart/,False,False,False,False
1jrbgqt,Data Engineering Performance -  Authors,I having worked in BI and transitioned to DE have followed best practices reading books by authors like Ralph Kimball in BI. Is there someone in DE with a similar level of reputation. I am not looking for specific technologies but rather want to pick up DE fundamentals especially in the performance and optimization space.,1,0,1,Amar_K1,2025-04-04 13:08:48,https://www.reddit.com/r/dataengineering/comments/1jrbgqt/data_engineering_performance_authors/,False,False,False,False
1jravos,Unstructured Data,"I see this has been asked prior but I didn't see a clear answer. We have a smallish database (glorified spreadsheet) where one field contains text. It houses details regarding customers, etc calling in for various issues. For various reasons (in-house) they want to keep using the simple app (it's a SharePoint List). I can easily download the data to a CSV file, for example, but is there a fairly simple method (AI?) to make sense of this data and correlate it? Maybe a creative prompt? Or is there a tool for this? (I'm not a software engineer). Thanks!",1,0,1,Top_Sink9871,2025-04-04 12:39:42,https://www.reddit.com/r/dataengineering/comments/1jravos/unstructured_data/,False,False,False,False
1jr6oc9,Do you need statistics to land a DE job?,"As the title suggests. Even if stats are not used on the job, will having stats qualifications give me an edge in the hiring process?",2,0,13,Normal-Bandicoot-180,2025-04-04 08:03:59,https://www.reddit.com/r/dataengineering/comments/1jr6oc9/do_you_need_statistics_to_land_a_de_job/,False,False,False,False
1jr05id,[Seeking Guidance] Aspiring GCP Data Engineer – Will Work Pro Bono for Hands-On Experience!,"Hey r/dataengineering community,  

I’m deep into prepping for the Google Cloud Professional Data Engineer cert and want to transition from theory to real-world projects. To ace the exam and build job-ready skills, I’m looking for:  

- Hands-on opportunities (pro bono!) to work with GCP tools like BigQuery, Dataflow, Pub/Sub, Cloud Composer, etc.  
- Mentorship or collaboration on data pipelines, workflow optimization, or cloud architecture projects.  
- Open-source/community projects needing an extra pair of hands.  

Why me? I’m motivated, detail-oriented, and eager to learn. I’ll treat your project like my own!  

If you’re working on anything data-related in GCP - or know someone who is - I’d hugely appreciate a chance to contribute (or even just advice on where to start). Comment/DM me, and thanks for being an awesome community!  

P.S. Upvotes for visibility help a ton! 🙏",1,0,3,aiqdec,2025-04-04 01:33:00,https://www.reddit.com/r/dataengineering/comments/1jr05id/seeking_guidance_aspiring_gcp_data_engineer_will/,False,False,False,False
1jqr8c7,SSIS resources and it's contribution to career,"I recently finished an internship where I worked with C#, .NET, and AWS, and I really want to focus more on cloud technologies. But at my current company, I’ve been asked to work with SSIS and become the go-to person when issues come up. They do have plans to move to cloud-native ETL solutions, but for now, SSIS is a priority.

I’m worried that I’m getting further from working with cloud and might get stuck with SSIS, which doesn’t seem to have as many resources or an active community compared to cloud-based alternatives. I don’t want to limit my career growth by focusing too much on something that could be phased out.

Has anyone been in a similar situation? How did you balance working with older tech while keeping up with modern cloud tools? Also, any good SSIS resources you’d recommend? Would appreciate any advice!",1,0,5,agap-0251,2025-04-03 19:09:41,https://www.reddit.com/r/dataengineering/comments/1jqr8c7/ssis_resources_and_its_contribution_to_career/,False,False,False,False
1jqm173,Intern working on data quality/anomaly detection — looking for ideas & tech suggestions,"Hey folks, I'm currently interning at an e-commerce company where my main focus is on **data quality and anomaly detection** in our tracking pipeline.

We're using **SQL and Python** to write basic data quality checks (like % of nulls, value ranges, row counts, etc.), and they run in **Airflow** every time the pipeline executes. Our stack is mostly **AWS Lambda → Airflow → Redshift**, and the data comes from real-time tracking of user events like **clicks, add-to-carts**, etc.

I want to go beyond basic checks and implement **time series anomaly detection**, especially for things like sudden spikes or drops in event volume. The challenge is I don't have labeled training data — just access to historical values.

I’ve considered:

* **Isolation Forest** (seems promising)
* **Prophet** (but it requires labeled data)
* **z-score** (a bit too naive/simple)

I'm thinking of an **unsupervised learning approach** and would love to hear from anyone who has done similar work in production. Are there any tools, libraries, or patterns you'd recommend? Bonus points if it fits well into an Airflow-based workflow.

Also… real talk: I’d love to impress the team and hopefully get hired full-time after this internship 😅 Any suggestions are welcome!

Thanks!",1,0,4,hehe1234zz,2025-04-03 15:51:49,https://www.reddit.com/r/dataengineering/comments/1jqm173/intern_working_on_data_qualityanomaly_detection/,False,False,False,False
1jqlgz1,Practical advice/resources for data engineering in digital transformation?,"I’m coming from a data analyst background — mostly worked on DWD layer and above (modeling, analytics, etc.). Recently talked to a few companies going through digital transformation, and they expect data roles to handle pulling data from source systems into the ODS layer (and then to DWD and above layers) as well. 

This is where I’m lacking experience. I get asked a lot of practical questions in interviews, like:

	•	How do you align with business/system owners who have no technical background at all?

	•	How do you confirm which fields to bring in, how to handle edge cases, or define how to treat anomalies?

	•	How do you make sure the raw data is good enough for future modeling?

I’d really appreciate practical resources (blogs, real-world case studies, anything hands-on) that help with this kind of work, especially around communication with non-technical stakeholders and defining raw data layers.

Any suggestions? Thanks!
",1,0,2,ok_effect_6502,2025-04-03 15:29:44,https://www.reddit.com/r/dataengineering/comments/1jqlgz1/practical_adviceresources_for_data_engineering_in/,False,False,False,False
1jql5rf,How Do You Handle Delta Load for Archival in Azure SQL?,"Hey everyone,

I’m currently architecting an archival solution and could use some seasoned advice on implementing delta load or CDC between two Azure SQL databases.

**Project Overview:**

* Our live database is becoming quite heavy. To manage this, we plan to enforce a 3-month retention policy on our 6 primary tables—meaning only the most recent 3 months of data will remain in production, while older data will be offloaded to an archive database.
* In addition, we have about 50 other tables that aren’t subject to archiving but still require a reliable delta load process.

**The Challenge:**

* Management is hesitant to use the CDC preview feature in Azure Data Factory due to cost concerns.
* A watermark column strategy isn’t viable either, as some of our tables lack a consistent `updateddate` field.

Given these constraints, I’m considering using change tracking. Do you think this is the best approach for our scenario? Or are there other tried-and-tested methods for implementing delta loading/CDC between Azure SQL databases that might better suit our requirements?

I’d appreciate any insights, alternative strategies, or best practices you’ve encountered in similar projects.

Thanks in advance for your input!

Looking forward to your thoughts.",1,0,0,ryanwolfh,2025-04-03 15:17:37,https://www.reddit.com/r/dataengineering/comments/1jql5rf/how_do_you_handle_delta_load_for_archival_in/,False,False,False,False
1jqqvau,Anyone attending snowflake summit 2025 in San Francisco?,"Hello there, I am attending snowflake summit in san francisco. If anyone attending this? If yes, what are the things you are looking for?
How was it last time? Any tips or tricks you can share?",0,0,1,Icy-Manufacturer3236,2025-04-03 18:56:08,https://www.reddit.com/r/dataengineering/comments/1jqqvau/anyone_attending_snowflake_summit_2025_in_san/,False,False,False,False
1jqle02,can we print current branch name (feature branch / master) inside databricks Notebook,"Hi Folks,

I am using Azure Databricks.

I wanted to know if we can we print current branch name (feature branch / master) inside databricks Notebook.

Thanks",0,0,2,nifty60,2025-04-03 15:26:30,https://www.reddit.com/r/dataengineering/comments/1jqle02/can_we_print_current_branch_name_feature_branch/,False,False,False,False
1jqurgc,What type of risks do you take at work?,"I've recently been asked why I'm hesitant to go forward with a certain idea. I responded that our team has never tried that and we should be cautious to the higher up who was proposing the idea. He saw what I was getting at. It made me wonder if risk taking in this domain is something people do often. So, do you take any risks at work? If so, how often and what kind?",0,0,8,Traditional_Reason59,2025-04-03 21:26:22,https://www.reddit.com/r/dataengineering/comments/1jqurgc/what_type_of_risks_do_you_take_at_work/,False,False,False,False
1jqqbgt,Data Platform Engineer,"I have fiddled with Snowflake and dbt, but it was more hit and trial than any focused guide. Anyone in this group who have experience in insurance can guide me on sample questions and specially related to Snowflake - Platform Administration & Performance Optimization, Cost Management & Security Compliance.",0,0,1,srijit43,2025-04-03 18:34:50,https://www.reddit.com/r/dataengineering/comments/1jqqbgt/data_platform_engineer/,False,False,False,False
1jqqm5h,Is it reasonable to expect flawless work from juniors?,"Hello! I’m a junior Data Engineer at a bank, I’ve been working here for the last 6 months and currently I’m working in a project that turned to be harder than expected.

The thing is, I tend to make mistakes regarding data validation, some of the logics or values appear as null due to my own mistakes, I am accountable for them and fix them as soon as they are reported back to me, but lately my boss has been pushing me to send perfect code, and if a mistake is found, I get reprimanded.

What I don’t know, is it reasonable for me to never make any mistake? Are my boss expectations too unrealistic? If I should make no mistakes, do you have any tips to prevent errors in my code?",0,0,24,Responsible-Cow2572,2025-04-03 18:46:18,https://www.reddit.com/r/dataengineering/comments/1jqqm5h/is_it_reasonable_to_expect_flawless_work_from/,False,False,False,False
