# ğŸš€ **RedditFlow**  
### *A Scalable ETL Pipeline with Airflow, AWS, and Redshift*

**RedditFlow** is a powerful, scalable ETL pipeline built to extract Reddit data, transform it using AWS services, and load it into an Amazon Redshift data warehouse â€” all orchestrated using Apache Airflow. It's designed for cloud-native data engineering and analytics workflows.


## ğŸ“– Overview
Overview
RedditFlow enables you to:

1. ğŸ” **Extract** data from Reddit via its official API  
2. ğŸ“¦ **Store** raw JSON data in **Amazon S3** using Airflow DAGs  
3. ğŸ”„ **Transform** and catalog data with **AWS Glue** and **Athena**  
4. ğŸš€ **Load** clean, structured data into **Amazon Redshift** for querying and analytics  

Perfect for data engineers looking to scale Reddit data pipelines in the cloud!

---

## ğŸ—ï¸ Architecture

![RedditDataEngineering](https://github.com/user-attachments/assets/6f2481c8-8dad-41f1-8e03-798b2714f199)

### ğŸ”§ **Components:**

- **ğŸ“¡ Reddit API** â€“ Source of posts, comments, metadata  
- **ğŸ› ï¸ Apache Airflow + Celery** â€“ Workflow orchestration and task distribution  
- **ğŸ˜ PostgreSQL** â€“ Airflow metadata DB and optional staging area  
- **ğŸª£ Amazon S3** â€“ Raw data lake for storing extracted files  
- **ğŸ§¬ AWS Glue** â€“ ETL and data cataloging engine  
- **ğŸ“Š Amazon Athena** â€“ Serverless SQL queries on S3 data  
- **ğŸ¢ Amazon Redshift** â€“ Scalable, final data warehouse for analysis  

---

## âš™ï¸ Prerequisites

Before you begin, make sure you have:

- âœ… An AWS Account with permissions for **S3**, **Glue**, **Athena**, and **Redshift**
- âœ… Reddit API credentials (client ID & secret)
- âœ… **Docker** installed
- âœ… **Python 3.9+**

---

## ğŸ§ª System Setup

### 1. ğŸš€ Clone the repository

```bash
git clone https://github.com/Ajay1812/Reddit_DataEngineering.git
cd RedditDataEngineering
```

### 2. ğŸ Create and activate a virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3. ğŸ“¦ Install dependencies

```bash
pip install -r requirements.txt
```

### 4. âš™ï¸ Configure your environment

Edit `config.conf` and add your credentials and settings:

```ini
[database]
database_host = localhost
database_name = airflow_reddit
database_port = 5432
database_username = postgres
database_password = postgres

[file_paths]
input_path = /opt/airflow/data/input
output_path = /opt/airflow/data/output

[api_keys]
reddit_secret_key = [SECRET KEY HERE]
reddit_client_id = [CLIENT ID HERE]

[aws]
aws_access_key_id = [AWS ACCESS KEY ID]
aws_secret_access_key = [AWS SECRET ACCESS KEY]
aws_session_token = [AWS SESSION TOKEN]
aws_region = [AWS REGION]
aws_bucket_name = [S3 BUCKET NAME]

[etl_settings]
batch_size = 100
error_handling = abort
log_level = info
```

### 5. ğŸ³ Start Docker containers

```bash
docker-compose up -d
```

### 6. ğŸŒ Open the Airflow UI

```bash
open http://localhost:8080
```

---

## ğŸ“¬ Contact

Got questions or want to contribute? Feel free to reach out!

- ğŸ“§ **Email**: [a.kumar01c@gmail.com](mailto:a.kumar01c@gmail.com)  
- ğŸ”— **GitHub**: [Ajay1812](https://github.com/Ajay1812)

---

### ğŸ‰ Happy Data Engineering!

Let the data flow ğŸš€ğŸ“ˆ
